{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GraceUmutesi/NLP-fellowship-assignment/blob/main/Word_and_PDF_extraction_Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Word Documents and PDFs\n",
        "\n",
        "In the African continent, most documents are in word or PDF. For the data in the documents to be processed and used in AI, they need to be extracted. To do this, the data will be extracted to txt format.\n",
        "\n"
      ],
      "metadata": {
        "id": "-1LUjCesZBKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package\n",
        "The package to be used in textract.\n",
        "\n",
        "To install the package, use the commands below\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "pip install textract\n",
        "```\n",
        "\n",
        "The package supports a lot of formats including but not limmited:\n",
        "\n",
        "csv, doc, docx, epub,json, html,pdf, jpg, pptx, xls, xlsx, ogg \n",
        "\n"
      ],
      "metadata": {
        "id": "YhWTdFi4ebY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract"
      ],
      "metadata": {
        "id": "rTyZY41XgWwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "841a5445-b2cc-41a1-e870-0f0a57300796"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 10.8 MB/s \n",
            "\u001b[?25hCollecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 213 kB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 65.3 MB/s \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.9.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tzdata\n",
            "  Downloading tzdata-2022.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 69.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=87e84d4bb752e154f4612301d69b9f3562d78e95926dd80238ec09c3567418f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=bcadf0246b1f5302dceac6fba4c0e8e446035b05fa665e175127479fafbc3c7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=2300568a6f30d524010d400bd0989a3a8301bef7ebfd6230bf533164b1506aa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=4ef5308df077f6a9f0f5dcfb1a1ef5009a8b137cfa6a1179f408b55214a1d3c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, soupsieve, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 soupsieve-2.3.2.post1 textract-1.6.5 tzdata-2022.5 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rx5KhiWzQVRQ"
      },
      "outputs": [],
      "source": [
        "import textract\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wil first mount the google drive. This will give us acces to my drive folder where we can find files"
      ],
      "metadata": {
        "id": "4n65qBvfljQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqM410Q_hQBr",
        "outputId": "bd4d546e-0b3f-4614-833a-282ba970871b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the ditection to where the files are located\n",
        "path = '/content/gdrive/MyDrive/NLP fellowship/assignments/day3/'\n",
        "os.chdir(path)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaX6mmVBjdlw",
        "outputId": "3e5493e3-a6c8-4460-c036-e61af2d1ed70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Banana Bread.pdf'\t\t      text\n",
            " extracted.txt\t\t\t      Word_and_PDF_extraction_Day3.ipynb\n",
            "'PDF NLP parliamentary proceedings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code for extracting text pdf\n",
        "text = textract.process('Banana Bread.pdf')\n",
        "text = text.decode(\"utf-8\")\n",
        "type(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIZ2uzj4gLLP",
        "outputId": "e9068ac7-36df-47b5-ffc8-596f2ed9bd06"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+'/text/textract_file.txt', 'w',  encoding='utf-8') as y:\n",
        "  for x in text.split('\\n'): #get every single line\n",
        "      if x != '':\n",
        "          y.write(x+' \\n')\n",
        "y"
      ],
      "metadata": {
        "id": "CPiyJT0GrNKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af9fd14-e3e8-491a-af0f-0aa76aaa2f86"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/gdrive/MyDrive/NLP fellowship/assignments/day3//text/textract_file.txt' mode='w' encoding='utf-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(path +\"/text\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2krIkgxQsW8m",
        "outputId": "87d5c73e-8c08-4c2d-b99d-ea5d5079f57b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textract_file.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting text with python package\n",
        "Docx files are writen in xml under the hood. If you unzip a docx file, you will find the components. The text is found in an xml file named document.xml. From the xml, you can extract text using beautifulsoup."
      ],
      "metadata": {
        "id": "xKU2FIS4vK4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "content = []\n",
        "\n",
        "\n",
        "xml = 'document.xml'\n",
        "\n",
        "with open(xml, \"r\") as file:\n",
        "    # Read each line in the file, readlines() returns a list of lines\n",
        "    content = file.readlines()\n",
        "    # Combine the lines in the list into a string\n",
        "    content = \"\".join(content)\n",
        "    bs_content = bs(content, \"lxml\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(bs_content.prettify())\n"
      ],
      "metadata": {
        "id": "TZ1MqQnWcLXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMRT_txBgDA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In class practicals\n",
        "From the xml, extract the text. The text should be put in a text file and have each sentence should be in its own line (No spaces between each line). e.g\n",
        "\n",
        "this is a book.\n",
        "\n",
        "the book looks nice.\n"
      ],
      "metadata": {
        "id": "KDcIOuhav13T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "You will be given 5 parliamentary proceedings, extract the text and put them in one txt file. Ski[p the last and first page. Steps:\n",
        "\n",
        "\n",
        "1.   Get a list of all the filepaths (through code)\n",
        "2.   open the file you want to write to(Note, remember to append the text)\n",
        "3.   Loop through every file\n",
        "4.   extract the text\n",
        "5.   write to the file (each sentence will be on its own line). There should be no spaces between lines (Note: remove empty lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "ycw-85mfyM_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9pqkWqOYnta",
        "outputId": "1f6c0f30-ea5c-421f-a5f1-df5609cf09ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.11.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# py.pdf4 for removing the first and the last page\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "\n",
        "def split(path, name_of_split):\n",
        "    pdf = PdfFileReader(path)\n",
        "    for page in range(pdf.getNumPages()):\n",
        "        pdf_writer = PdfFileWriter()\n",
        "        pdf_writer.addPage(pdf.getPage(page))\n",
        "        # print(pdf_writer)\n",
        "        output = f'{name_of_split}{page}.pdf'\n",
        "        with open(output, 'wb') as output_pdf:\n",
        "            pdf_writer.write(output_pdf)\n",
        "        print(pdf_writer)"
      ],
      "metadata": {
        "id": "tDgI7bgpgI7Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/gdrive/MyDrive/NLP fellowship/assignments/day3/PDF NLP parliamentary proceedings/Hansard Report - Thursday 6th October 2022 (P).pdf'\n",
        "name='doc1'\n",
        "new_doc=split(path,name)\n",
        "new_doc"
      ],
      "metadata": {
        "id": "hMR3UT2rhVYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# py.pdf4 for removing the first and the last page\n",
        "\n",
        "file_path=[]\n",
        "path='/content/gdrive/MyDrive/NLP fellowship/assignments/day3/PDF NLP parliamentary proceedings/'\n",
        "for single_file in os.listdir(path):\n",
        "  if single_file.endswith('.pdf'):\n",
        "    f_path=os.path.join(path + single_file)\n",
        "    file_path.append(f_path)\n",
        "file_path\n",
        "for single_path in file_path:\n",
        "  text =textract.process(single_path)\n",
        "  text = text.decode(\"utf-8\")\n",
        "\n",
        "with open('/content/gdrive/MyDrive/NLP fellowship/assignments/day3/extracted.txt','w+', encoding='utf-8') as f:\n",
        "  for x in text.split('\\n'):\n",
        "    if x!='':\n",
        "      f.write(x+ ' \\n')\n"
      ],
      "metadata": {
        "id": "KMAEBzOkqSEM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d1C2yQn8S8Au"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Us60gRAiTMea"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}